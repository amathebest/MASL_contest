zeros <- sample(zeros, nshow)
col <- gray.colors(nshow)
plot(density(beta.hat[,altro[1]]), xlim=c(-.2,.5), ylim=c(0,30))
plot(density(beta.hat[,zeros[1]]), xlim=c(-.2,.5), ylim=c(0,30))
plot(density(beta.hat[,zeros[1]]), xlim=c(-.3,.3), ylim=c(0,30))
for (i in 2:length(altro)) {
lines(density(beta.hat[,altro[i]]), col=colore[i])
}
for (i in 2:length(zeros)) {
lines(density(beta.hat[,zeros[i]]), col=colore[i])
}
for (i in 2:length(zeros)) {
lines(density(beta.hat[,zeros[i]]), col=col[i])
}
data("bodyfat", package = "TH.data")
install.packages("TH.data")
data("bodyfat", package = "TH.data")
head(bodyfat)
library(glasso)
install.packages("glasso")
View(data)
BFmod0 <- glasso(cor(bodyfat), rho = 0.9)# modify rho
library(glasso)
BFmod0 <- glasso(cor(bodyfat), rho = 0.9)# modify rho
BFmod0 <- glasso(cor(bodyfat), nobs = nrow(bodyfat), rho = 0.9, penalize.diagonal = F)# modify rho
# takes var/cov matrix, and rho -> lambda, reg. estimator.
BFmod0$loglik
round(BFmod0$wi, 3)
data("bodyfat", package = "TH.data")
head(bodyfat)
BFmod0 <- glasso(cor(bodyfat), nobs = nrow(bodyfat), rho = 0.9, penalize.diagonal = F)# modify rho
# takes var/cov matrix, and rho -> lambda, reg. estimator.
BFmod0$loglik
round(BFmod0$wi, 3)
BFmod0 <- glasso(cor(bodyfat), nobs = nrow(bodyfat), rho = 0.5, penalize.diagonal = F)# modify rho
# takes var/cov matrix, and rho -> lambda, reg. estimator.
BFmod0$loglik
round(BFmod0$wi, 3) # we put the shrinkage on the off-diagonal elements of the inverse of the variance/covariance matrix
grafo <- matrix(as.numeric(BFmod0$wi !=0), nrow = 10)
grafo
diag(grafo) <- 0
drawGraph(grafo)
library(ggm)
colnames(grafo) <- rownames(grafo) <- colnames(bodyfat)
drawGraph(grafo)
library(glasso)
data("bodyfat", package = "TH.data")
head(bodyfat)
BFmod0 <- glasso(cor(bodyfat), nobs = nrow(bodyfat), rho = 0.5, penalize.diagonal = F)# modify rho
# takes var/cov matrix, and rho = lambda, reg. estimator.
BFmod0$loglik
round(BFmod0$wi, 3) # we put the shrinkage on the off-diagonal elements of the inverse of the variance/covariance matrix
grafo <- matrix(as.numeric(BFmod0$wi !=0), nrow = 10)
grafo
diag(grafo) <- 0
library(ggm)
colnames(grafo) <- rownames(grafo) <- colnames(bodyfat)
drawGraph(grafo)
stima <- ggm:fitConGraph(grafo, S = var(bodyfat), n = 71)
stima <- ggm::fitConGraph(grafo, S = var(bodyfat), n = 71)
stima
library(gRbase)
library(gRain)
library(Rgraphviz)
#### How to construct a DAG
dag0 <- dag(~a, ~b * a, ~c * a * b, ~d * c * e, ~e * a)
edgeList(dag0) # when the class of edges is "oriented", every edge is in the form of <parent> <child>
#### How to construct a DAG
dag0 <- dag(~a, ~b * a, ~c * a * b, ~d * c * e, ~e * a)
Rgraphviz::plot(dag0, cex.main = 0.5) # note how the script works: ~c * a * b = c depends on both a and b
dag0
edgeList(dag0) # when the class of edges is "oriented", every edge is in the form of <parent> <child>
as.adjMAT(dag0) # adjacency matrix, on rows = parents, on columns = child
# these two functions identify the parents or the children of a specified node
parents("d", dag0)
children("a", dag0)
# check: b ind e?
ancestralSet(c("b", "e"), dag0)
iplot(ancestralGraph(c("b", "e"), dag0))
iplot(moralize(ancestralGraph(c("b", "e"), dag0))) # --> no
library(gRbase)
library(gRain)
library(Rgraphviz)
#### How to construct a DAG
dag0 <- dag(~a*b,~a*c,~c*b,~b*d,~a*e,~e*d,~c*f,~b*f)
Rgraphviz::plot(dag0, cex.main = 0.5) # note how the script works: ~c * a * b = c depends on both a and b
#### How to construct a DAG
dag0 <- dag(~b*a,~c*a,~b*c,~d*b,~e*a,~d*e,~f*c,~f*b)
Rgraphviz::plot(dag0, cex.main = 0.5) # note how the script works: ~c * a * b = c depends on both a and b
edgeList(dag0) # when the class of edges is "oriented", every edge is in the form of <parent> <child>
# check: b ind e?
ancestralSet(c("c"), dag0)
#########  Data genetation
# change seetting as you prefer
n <- 200 # change to 100, 500
set.seed(26)
X1 <- round(rnorm(n),1)
X2 <- round(rt(n, df=6), 1)
X3 <- rbinom(n,1,.6) # attention to set the factors if not binary
Y <- X1^2 - 2*X1*X2 + 2*X1^X3 - X1*I(X2>0) + round(rnorm(n),1)
#########  Data genetation
# change seetting as you prefer
n <- 500 # change to 100, 500
set.seed(26)
X1 <- round(rnorm(n), 1)
X2 <- round(rt(n, df = 6), 1) # t distribution
X3 <- rbinom(n, 1, .6) # attention to set the factors if not binary, bernoulli distribution
Y <- X1^2 - 2*X1*X2 + 2*X1^X3 - X1*I(X2>0) + round(rnorm(n),1)
hist(X1)
hist(X2)
hist(x3)
hist(X3)
e <- round(rnorm(n), 1)
Y <- X1^2 - 2*X1*X2 + 2*X1^X3 - X1*I(X2>0) + e
plot(Y)
plot(density(Y))
## ordinary regression
mod0 <- lm(Y ~ X1 + X2 + X3)
summary(mod0)
# this is the true regression model, assumed hierarchical.
# use : instead of * to have exactly the true model
mod0.true <- lm(Y~I(X1^2) + X1*X2 + I(X1^X3) + X1*I(X2>0))
summary(mod0.true)
Y.decili <- quantile(Y, 0:10/10)
Y.decili
soglie  <- cut(Y, Y.decili, include.lowest=TRUE)
soglie
plot(X1, X2, col=grey(10:2/15)[soglie], pch=20, xlab="X1",ylab="X2")
mod12 <- tree::tree(Y~X1+X2)
install.packages("tree")
mod12 <- tree::tree(Y~X1+X2)
tree::partition.tree(mod12, add=TRUE)
mod12
install.packages("rpart")
mod1 <- rpart(Y ~ X1 + X2 + X3, control = rpart.control(cp = 0.0, minsplit = 10))
####### fit a regression tree on our data
library(rpart)
mod1 <- rpart(Y ~ X1 + X2 + X3, control = rpart.control(cp = 0.0, minsplit = 10))
rpart.plot::prp(mod1)
install.packages("rpart.plot")
library(rpart.plot)
rpart.plot::prp(mod1)
X3
as.factor(X3)
summary(mod1)# too long - see it on a shorter tree
?rpart.object
mod1$frame
rpart.plot::prp(mod1, extra = 1, type = 1)
#########  Data genetation
# change seetting as you prefer
n <- 100 # change to 100, 500
set.seed(26)
X1 <- round(rnorm(n), 1)
X2 <- round(rt(n, df = 6), 1) # t distribution
X3 <- rbinom(n, 1, .6) # attention to set the factors if not binary, bernoulli distribution
e <- round(rnorm(n), 1)
Y <- X1^2 - 2*X1*X2 + 2*X1^X3 - X1*I(X2>0) + e
## ordinary regression
mod0 <- lm(Y ~ X1 + X2 + X3) # --> restricting g(x) to be found in the family of linear functions
# of course this is wrong as an assumption, as the relation between X1, X2, X3 and Y is not linear
summary(mod0) # this assumes that the model is true, so when the assumption is wrong, also the answer
# this is the true regression model, assumed hierarchical.
# use : instead of * to have exactly the true model
mod0.true <- lm(Y ~ I(X1^2) + X1*X2 + I(X1^X3) + X1*I(X2>0))
summary(mod0.true)
Y.decili <- quantile(Y, 0:10/10)
Y.decili
soglie  <- cut(Y, Y.decili, include.lowest=TRUE)
soglie
plot(X1, X2, col=grey(10:2/15)[soglie], pch=20, xlab="X1",ylab="X2")
mod12 <- tree::tree(Y~X1+X2)
tree::partition.tree(mod12, add=TRUE)
mod12
####### fit a regression tree on our data
library(rpart)
library(rpart.plot)
mod1 <- rpart(Y ~ X1 + X2 + X3, control = rpart.control(cp = 0.0, minsplit = 10)) # we say to the procedure to avoid
rpart.plot::prp(mod1)
summary(mod1)# too long - see it on a shorter tree
?rpart.object
mod1$frame
mean((Y[X1<2.2] - mean(Y[X1<2.2]))^2) + mean((Y[X1>=2.2] - mean(Y[X1>=2.2]))^2)
mean((Y[X3==1] - mean(Y[X3==1]))^2) + mean((Y[X3==0] - mean(Y[X3==0]))^2)
rpart.plot::prp(mod1, extra = 1, type = 1)
mod1 <- rpart(Y ~ X1 + X2 + X3, control = rpart.control(cp = 0.0, minsplit = 30)) # we say to the procedure to avoid
rpart.plot::prp(mod1)
summary(mod1)# too long - see it on a shorter tree
?rpart.object
mod1$frame
mean((Y[X1<2.2] - mean(Y[X1<2.2]))^2) + mean((Y[X1>=2.2] - mean(Y[X1>=2.2]))^2)
mean((Y[X3==1] - mean(Y[X3==1]))^2) + mean((Y[X3==0] - mean(Y[X3==0]))^2)
rpart.plot::prp(mod1, extra = 1, type = 1)
#########  Data genetation
# change seetting as you prefer
n <- 200 # change to 100, 500
set.seed(26)
X1 <- round(rnorm(n), 1)
X2 <- round(rt(n, df = 6), 1) # t distribution
X3 <- rbinom(n, 1, .6) # attention to set the factors if not binary, bernoulli distribution
e <- round(rnorm(n), 1)
Y <- X1^2 - 2*X1*X2 + 2*X1^X3 - X1*I(X2>0) + e
## ordinary regression
mod0 <- lm(Y ~ X1 + X2 + X3) # --> restricting g(x) to be found in the family of linear functions
# of course this is wrong as an assumption, as the relation between X1, X2, X3 and Y is not linear
summary(mod0) # this assumes that the model is true, so when the assumption is wrong, also the answer
# this is the true regression model, assumed hierarchical.
# use : instead of * to have exactly the true model
mod0.true <- lm(Y ~ I(X1^2) + X1*X2 + I(X1^X3) + X1*I(X2>0))
summary(mod0.true)
Y.decili <- quantile(Y, 0:10/10)
Y.decili
soglie  <- cut(Y, Y.decili, include.lowest=TRUE)
soglie
plot(X1, X2, col=grey(10:2/15)[soglie], pch=20, xlab="X1",ylab="X2")
mod12 <- tree::tree(Y~X1+X2)
tree::partition.tree(mod12, add=TRUE)
mod12
####### fit a regression tree on our data
library(rpart)
library(rpart.plot)
mod1 <- rpart(Y ~ X1 + X2 + X3, control = rpart.control(cp = 0.0, minsplit = 30)) # we say to the procedure to avoid
rpart.plot::prp(mod1)
summary(mod1)# too long - see it on a shorter tree
?rpart.object
mod1$frame
mean((Y[X1<2.2] - mean(Y[X1<2.2]))^2) + mean((Y[X1>=2.2] - mean(Y[X1>=2.2]))^2)
mean((Y[X3==1] - mean(Y[X3==1]))^2) + mean((Y[X3==0] - mean(Y[X3==0]))^2)
rpart.plot::prp(mod1, extra = 1, type = 1)
## PRUNING
mod1$cptable # norice that this values are computed using a 10-fol CV
plotcp(mod1)
# the prooned tree has to be below the h line
abline(v = which.min(mod1$cptable[,4]), col = 'mediumpurple4') # notice trade-off accuracy-interpretability
abline(v = 3, col = 'mediumpurple1') # size 5
abline(v = 6, col = 'mediumpurple3') # size 12
abline(h = aa, col = 'mediumpurple3') # size 12
aa <- min(mod1$cptable[,4])*3.811789
abline(h = aa, col = 'mediumpurple3') # size 12
par(mfrow=c(1,2))
rsq.rpart( mod1) # plot the residual sum of squares
abline(v = mod1$cptable[6,2], col = 'mediumpurple1') # size 7
abline(v = mod1$cptable[which.min(mod1$cptable[,4]),2], col = 'mediumpurple4')
par(mfrow=c(1,1))
par(mfrow=c(1,1))
# rule of the thumb: relerror + xstd < xerror
plot(mod1$cptable[,2], mod1$cptable[,3], type="b", pch=20)
lines(mod1$cptable[,2],mod1$cptable[,3]+ mod1$cptable[,5], col="purple")
lines(mod1$cptable[,2],mod1$cptable[,4], col="maroon")
mod1$cptable[,3]+ mod1$cptable[,5] < mod1$cptable[,4]
mod1$cptable[3:5,]
# 1se rule
which.min(mod1$cptable[,4]+ mod1$cptable[,5])
## Prune the tree: according to the min xerror
mod1.pruned.long <- prune.rpart(mod1, cp = mod1$cptable[which.min(mod1$cptable[,4]),1] )
summary(mod1.pruned.long)
rpart.plot::prp(mod1.pruned.long, extra = 1, type = 1)
mod1.pruned <- prune.rpart(mod1, cp = 0.027)
summary(mod1.pruned)
rpart.plot::prp(mod1.pruned, extra = 1, type = 1)
# extract dummies function
library(rpart.utils)
rpart.lists(mod1.pruned)
rpart.subrules.table(mod1.pruned)
install.packages("rpart.utils")
brary(rpart)
library(MASS)
data(Boston)
head(Boston)
lm.out <- lm(medv ~ ptratio, data = Boston)
plot(medv ~ ptratio, data = Boston)
abline(lm.out, col="red")
fit <- rpart(medv ~ ptratio, data = Boston)
fit.pruned.tree <-prune(fit, cp = 0.05)
plot(fit.pruned.tree)
text(fit.pruned.tree, use.n = TRUE)
fit.pruned.tree
plot(medv ~ ptratio, data = Boston)
plot(fit.pruned.tree)
text(fit.pruned.tree, use.n = TRUE)
y
y
library(gRbase)
library(gRain)
library(Rgraphviz)
#### How to construct a DAG
dag0 <- dag(~b*a,~c*a,~b*c,~d*b,~e*a,~d*e,~f*c,~f*b)
Rgraphviz::plot(dag0, cex.main = 0.5) # note how the script works: ~c * a * b = c depends on both a and b
dag0
edgeList(dag0) # when the class of edges is "oriented", every edge is in the form of <parent> <child>
as.adjMAT(dag0) # adjacency matrix, on rows = parents, on columns = child
# these two functions identify the parents or the children of a specified node
parents("d", dag0)
children("a", dag0)
# moralized graph
ug0 <- moralize(dag0)
iplot(ug0)
# functions to get edges and cliques
edgeList(ug0)
getCliques(ug0)
# check: b ind e?
ancestralSet(c("a"), dag0)
iplot(ancestralGraph(c("b", "e"), dag0))
# check: b ind e?
ancestralSet(c("c"), dag0)
parents(c("d", "b"), dag0)
library(gRbase)
library(gRain)
library(Rgraphviz)
#### How to construct a DAG
dag0 <- dag(~b*a,~c*a,~b*c,~d*b,~e*a,~d*e,~f*c,~f*b)
Rgraphviz::plot(dag0, cex.main = 0.5) # note how the script works: ~c * a * b = c depends on both a and b
dag0
edgeList(dag0) # when the class of edges is "oriented", every edge is in the form of <parent> <child>
as.adjMAT(dag0) # adjacency matrix, on rows = parents, on columns = child
# these two functions identify the parents or the children of a specified node
parents("d", dag0)
children("a", dag0)
# moralized graph
ug0 <- moralize(dag0)
iplot(ug0)
# functions to get edges and cliques
edgeList(ug0)
getCliques(ug0)
# check: b ind e?
ancestralSet(c("b", "e"), dag0)
iplot(ancestralGraph(c("b", "e"), dag0))
iplot(moralize(ancestralGraph(c("b", "e"), dag0))) # --> no
# check: (d,e) ind b | c ?
ancestralSet(c("b", "e", "d", "c"), dag0)
(b, c) ind e
# check: (d,e) ind b | c ?
ancestralSet(c("b", "e", "d", "c"), dag0)
iplot(ancestralGraph(c("b", "e", "d", "c"), dag0))
iplot(moralize(ancestralGraph(c("b", "e", "d", "c"), dag0)))
# check: b ind e?
ancestralSet(c("b", "e"), dag0)
iplot(ancestralGraph(c("b", "e"), dag0))
iplot(moralize(ancestralGraph(c("b", "e"), dag0))) # --> no
# check: b ind e?
ancestralSet(c("b", "e"), dag0)
# check: b ind e?
ancestralSet(c("b", "e"), dag0)
iplot(ancestralGraph(c("b", "e"), dag0))
iplot(moralize(ancestralGraph(c("b", "e"), dag0))) # --> no
# check: a ind d
ancestralSet(c("a", "d"), dag0)
iplot(ancestralGraph(c("a", "d"), dag0))
iplot(moralize(ancestralGraph(c("a", "d"), dag0)))
# check: b ind e?
ancestralSet(c("b", "e"), dag0)
iplot(ancestralGraph(c("b", "e"), dag0))
iplot(moralize(ancestralGraph(c("b", "e"), dag0))) # --> no
library(gRbase)
library(gRain)
library(Rgraphviz)
#### How to construct a DAG
dag0 <- dag(~b*a,~c*a,~b*c,~d*b,~e*a,~d*e,~f*c,~f*b)
Rgraphviz::plot(dag0, cex.main = 0.5) # note how the script works: ~c * a * b = c depends on both a and b
dag0
edgeList(dag0) # when the class of edges is "oriented", every edge is in the form of <parent> <child>
as.adjMAT(dag0) # adjacency matrix, on rows = parents, on columns = child
# these two functions identify the parents or the children of a specified node
parents("d", dag0)
children("a", dag0)
# moralized graph
ug0 <- moralize(dag0)
iplot(ug0)
# functions to get edges and cliques
edgeList(ug0)
getCliques(ug0)
# check: b ind e?
ancestralSet(c("b", "e"), dag0)
iplot(ancestralGraph(c("b", "e"), dag0))
iplot(moralize(ancestralGraph(c("b", "e"), dag0))) # --> no
######################  CLASSIFICATION TREES
library(rpart)
par(mfrow = c(1,1))
data(kyphosis)
head(kyphosis)
fit1 <- rpart(Kyphosis ~ ., data = kyphosis,
parms=list(split = "information"))
fit1
summary(fit1)
plot(fit1)
rpart.plot::rpart.plot(fit1)
install.packages("randomForest")
require(randomForest)
require(MASS)
set.seed(101)
dim(Boston)
train=sample(1:nrow(Boston),300)
?Boston
rf.boston = randomForest(medv~. ,data = Boston, subset = train)
rf.boston
.err=double(13)
test.err=double(13)
for(mtry in 1:13){
fit=randomForest(medv~.,data=Boston,subset=train,mtry=mtry,ntree=400)
oob.err[mtry]=fit$mse[400]
pred=predict(fit,Boston[-train,])
test.err[mtry]=with(Boston[-train,],mean((medv-pred)^2))
cat(mtry," ")
}
oob.err=double(13)
test.err=double(13)
for(mtry in 1:13){
fit=randomForest(medv~.,data=Boston,subset=train,mtry=mtry,ntree=400)
oob.err[mtry]=fit$mse[400]
pred=predict(fit,Boston[-train,])
test.err[mtry]=with(Boston[-train,],mean((medv-pred)^2))
cat(mtry," ")
}
matplot(1:mtry,cbind(test.err,oob.err),pch=19,col=c("red","blue"),type="b",ylab="Mean Squared Error")
legend("topright",legend=c("OOB","Test"),pch=19,col=c("red","blue"))
require(gbm)
boost.boston=gbm(medv~.,data=Boston[train,],distribution="gaussian",n.trees=10000,shrinkage=0.01,interaction.depth=4)
install.packages("gbm")
require(gbm)
boost.boston=gbm(medv~.,data=Boston[train,],distribution="gaussian",n.trees=10000,shrinkage=0.01,interaction.depth=4)
summary(boost.boston)
##########################################
# BART
##########################################
rm(list=ls())
dyn.load('/Library/Java/JavaVirtualMachines/jdk1.8.0_45.jdk/Contents/Home/jre/lib/server/libjvm.dylib')
options(java.parameters="-Xmx5000m") # must be set initially
library(bartMachine)
install.packages("bartMachine")
set_bart_machine_num_cores(4)
library(ggm)
library(SIN)
# Data import
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
data <- read.table("esami_matem.txt", header = TRUE, sep = " ")
data$Anno <- NULL
# Variable correlation overview
cor(data)
head(data)
# SINful procedure that gives the p-values relative to the corresponding test on each edge
pvals <- sinUG(var(data), n = nrow(data), holm = T)
# Plotting the p-values of the tests for each edge
alpha_value = 0.15
plotUGpvalues(pvals, legend = F) # displays each edge with the corresponding p-value
abline(h = alpha_value, col = "blue")
# Adjacency matrix of the obtained graph
adj_mat <- getgraph(pvals, alpha = alpha_value, type = "UG")
# Plotting the obtained graph with the chosen value of alpha
drawGraph(getgraph(pvals, alpha = alpha_value, type = "UG"))
# Plotting the p-values of the tests for each edge
alpha_value = 0.15
plotUGpvalues(pvals, legend = F) # displays each edge with the corresponding p-value
abline(h = alpha_value, col = "blue")
# Plotting the obtained graph with the chosen value of alpha
drawGraph(getgraph(pvals, alpha = alpha_value, type = "UG"))
# Plotting the p-values of the tests for each edge
alpha_value = 0.30
plotUGpvalues(pvals, legend = F) # displays each edge with the corresponding p-value
abline(h = alpha_value, col = "blue")
# Adjacency matrix of the obtained graph
adj_mat <- getgraph(pvals, alpha = alpha_value, type = "UG")
# Plotting the obtained graph with the chosen value of alpha
drawGraph(getgraph(pvals, alpha = alpha_value, type = "UG"))
# Plotting the p-values of the tests for each edge
alpha_value = 0.15
plotUGpvalues(pvals, legend = F) # displays each edge with the corresponding p-value
abline(h = alpha_value, col = "blue")
# Adjacency matrix of the obtained graph
adj_mat <- getgraph(pvals, alpha = alpha_value, type = "UG")
# Plotting the obtained graph with the chosen value of alpha
drawGraph(getgraph(pvals, alpha = alpha_value, type = "UG"))
# Obtaining the graph definition that will be used to create the graph with my package
names <- rownames(adj_mat)
# Adjacency matrix of the obtained graph
adj_mat <- getgraph(pvals, alpha = alpha_value, type = "UG")
# Plotting the obtained graph with the chosen value of alpha
drawGraph(getgraph(pvals, alpha = alpha_value, type = "UG"))
# Analysis:
# We can assume that the first course (like Analysis 1) and the second one (like Analysis 2) are linked by
# an asymmetrical relationship, from the first to the second. For this reason we remove the edge pointing
# from the second to the first:
adj_mat["Analisi2", "Analisi1"] = 0
# The same approach is applied to the other courses:
adj_mat["Fisica2", "Fisica1"] = 0
adj_mat["Geometria2", "Geometria1"] = 0
# For the rest, in order to build a directed graph, we assume an ordering of the courses. This means that
# we decide which direction has a specific edge between two variables:
adj_mat["Geometria2", "Analisi2"] = 0
# For example, we can also assume that it's possible that Algebra gives proficiency on the exam Fisica1.
# For this reason we can direct the edge between Algebra and Fisica1 to be from Algebra and Fisica1, deleting
# the other direction:
adj_mat["Fisica1", "Algebra"] = 0
# By exploiting the same principle, we can assume an ordering also between Algebra and Geometria1:
adj_mat["Geometria1", "Algebra"] = 0
# Finally we direct two more edges with an arbitrary order:
adj_mat["Geometria2", "Analisi2"] = 0
adj_mat["MecRaz", "Geometria2"] = 0
# We adjust the final result by adding an edge from Fisica1 to Fisica2 since it makes sense to think that such
# an edge exist (also due to the fact that its p-value in the SINful procedure matrix is not that high). This is
# done also to prevent second type errors:
adj_mat["Fisica1", "Fisica2"] = 1
# Obtaining the graph definition that will be used to create the graph with my package
names <- rownames(adj_mat)
definition <- ""
for (i in 1:nrow(adj_mat)) {
for (j in 1:nrow(adj_mat)) {
if (adj_mat[i,j] == 1) {
new_edge <- paste(names[i], names[j], sep = "-")
definition <- paste(definition, new_edge, sep = ",")
}
}
}
definition
